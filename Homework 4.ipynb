{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112a0a64",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "## Professor Nathan Kutz\n",
    "## EE399\n",
    "## By: Gerin George\n",
    "\n",
    "Link to private GitHub Repo:\n",
    "\n",
    "https://github.com/gering92/EE399-Work\n",
    "\n",
    "Click on Homework 4 Writeup to see the writeup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e18dac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "81e269f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 81.0442\n",
      "Epoch [200/10000], Loss: 22.3072\n",
      "Epoch [300/10000], Loss: 65.6368\n",
      "Epoch [400/10000], Loss: 27.9052\n",
      "Epoch [500/10000], Loss: 14.5961\n",
      "Epoch [600/10000], Loss: 9.4846\n",
      "Epoch [700/10000], Loss: 7.2426\n",
      "Epoch [800/10000], Loss: 6.0671\n",
      "Epoch [900/10000], Loss: 5.3451\n",
      "Epoch [1000/10000], Loss: 4.8606\n",
      "Epoch [1100/10000], Loss: 4.5989\n",
      "Epoch [1200/10000], Loss: 4.4452\n",
      "Epoch [1300/10000], Loss: 4.3482\n",
      "Epoch [1400/10000], Loss: 4.2930\n",
      "Epoch [1500/10000], Loss: 4.2551\n",
      "Epoch [1600/10000], Loss: 4.2248\n",
      "Epoch [1700/10000], Loss: 4.1965\n",
      "Epoch [1800/10000], Loss: 4.1705\n",
      "Epoch [1900/10000], Loss: 4.1459\n",
      "Epoch [2000/10000], Loss: 4.1227\n",
      "Epoch [2100/10000], Loss: 4.1013\n",
      "Epoch [2200/10000], Loss: 4.0815\n",
      "Epoch [2300/10000], Loss: 4.0633\n",
      "Epoch [2400/10000], Loss: 4.0465\n",
      "Epoch [2500/10000], Loss: 4.0311\n",
      "Epoch [2600/10000], Loss: 4.0172\n",
      "Epoch [2700/10000], Loss: 4.0044\n",
      "Epoch [2800/10000], Loss: 3.9923\n",
      "Epoch [2900/10000], Loss: 3.9808\n",
      "Epoch [3000/10000], Loss: 3.9699\n",
      "Epoch [3100/10000], Loss: 3.9595\n",
      "Epoch [3200/10000], Loss: 3.9496\n",
      "Epoch [3300/10000], Loss: 3.9403\n",
      "Epoch [3400/10000], Loss: 3.9314\n",
      "Epoch [3500/10000], Loss: 3.9231\n",
      "Epoch [3600/10000], Loss: 3.9152\n",
      "Epoch [3700/10000], Loss: 3.9078\n",
      "Epoch [3800/10000], Loss: 3.9008\n",
      "Epoch [3900/10000], Loss: 3.8942\n",
      "Epoch [4000/10000], Loss: 3.8881\n",
      "Epoch [4100/10000], Loss: 3.8823\n",
      "Epoch [4200/10000], Loss: 3.8770\n",
      "Epoch [4300/10000], Loss: 3.8719\n",
      "Epoch [4400/10000], Loss: 3.8673\n",
      "Epoch [4500/10000], Loss: 3.8629\n",
      "Epoch [4600/10000], Loss: 3.8589\n",
      "Epoch [4700/10000], Loss: 3.8552\n",
      "Epoch [4800/10000], Loss: 3.8518\n",
      "Epoch [4900/10000], Loss: 3.8489\n",
      "Epoch [5000/10000], Loss: 3.8478\n",
      "Epoch [5100/10000], Loss: 3.8664\n",
      "Epoch [5200/10000], Loss: 4.0083\n",
      "Epoch [5300/10000], Loss: 3.9868\n",
      "Epoch [5400/10000], Loss: 3.9253\n",
      "Epoch [5500/10000], Loss: 3.9217\n",
      "Epoch [5600/10000], Loss: 3.9328\n",
      "Epoch [5700/10000], Loss: 3.9333\n",
      "Epoch [5800/10000], Loss: 3.9245\n",
      "Epoch [5900/10000], Loss: 3.9194\n",
      "Epoch [6000/10000], Loss: 3.9160\n",
      "Epoch [6100/10000], Loss: 3.9124\n",
      "Epoch [6200/10000], Loss: 3.9085\n",
      "Epoch [6300/10000], Loss: 3.9050\n",
      "Epoch [6400/10000], Loss: 3.9008\n",
      "Epoch [6500/10000], Loss: 3.8974\n",
      "Epoch [6600/10000], Loss: 3.8944\n",
      "Epoch [6700/10000], Loss: 3.8910\n",
      "Epoch [6800/10000], Loss: 3.8874\n",
      "Epoch [6900/10000], Loss: 3.8837\n",
      "Epoch [7000/10000], Loss: 3.8811\n",
      "Epoch [7100/10000], Loss: 3.8779\n",
      "Epoch [7200/10000], Loss: 3.8748\n",
      "Epoch [7300/10000], Loss: 3.8731\n",
      "Epoch [7400/10000], Loss: 3.8705\n",
      "Epoch [7500/10000], Loss: 3.8680\n",
      "Epoch [7600/10000], Loss: 3.8656\n",
      "Epoch [7700/10000], Loss: 3.8635\n",
      "Epoch [7800/10000], Loss: 3.8605\n",
      "Epoch [7900/10000], Loss: 3.8586\n",
      "Epoch [8000/10000], Loss: 3.8568\n",
      "Epoch [8100/10000], Loss: 3.8552\n",
      "Epoch [8200/10000], Loss: 3.8539\n",
      "Epoch [8300/10000], Loss: 3.8524\n",
      "Epoch [8400/10000], Loss: 3.8506\n",
      "Epoch [8500/10000], Loss: 3.8492\n",
      "Epoch [8600/10000], Loss: 3.8468\n",
      "Epoch [8700/10000], Loss: 3.8456\n",
      "Epoch [8800/10000], Loss: 3.8450\n",
      "Epoch [8900/10000], Loss: 3.8436\n",
      "Epoch [9000/10000], Loss: 3.8435\n",
      "Epoch [9100/10000], Loss: 3.8419\n",
      "Epoch [9200/10000], Loss: 3.8410\n",
      "Epoch [9300/10000], Loss: 3.8403\n",
      "Epoch [9400/10000], Loss: 3.8391\n",
      "Epoch [9500/10000], Loss: 3.8385\n",
      "Epoch [9600/10000], Loss: 3.8374\n",
      "Epoch [9700/10000], Loss: 3.8356\n",
      "Epoch [9800/10000], Loss: 3.8359\n",
      "Epoch [9900/10000], Loss: 3.8344\n",
      "Epoch [10000/10000], Loss: 3.8349\n",
      "Least-square error on training data: 3.8342\n",
      "Least-square error on test data: 20.5051\n"
     ]
    }
   ],
   "source": [
    "# Part One\n",
    "\n",
    "X = np.arange(0, 31)\n",
    "Y = np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "              40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])\n",
    "\n",
    "train_X = torch.tensor(X[:20], dtype=torch.float32).view(-1, 1)\n",
    "train_Y = torch.tensor(Y[:20], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "test_X = torch.tensor(X[20:], dtype=torch.float32).view(-1, 1)\n",
    "test_Y = torch.tensor(Y[20:], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(train_X)\n",
    "    loss = criterion(outputs, train_Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_pred = net(train_X)\n",
    "    train_error = criterion(train_pred, train_Y)\n",
    "    print(\"Least-square error on training data: {:.4f}\".format(train_error.item()))\n",
    "\n",
    "    test_pred = net(test_X)\n",
    "    test_error = criterion(test_pred, test_Y)\n",
    "    print(\"Least-square error on test data: {:.4f}\".format(test_error.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d3510b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 1338.1541\n",
      "Epoch [200/10000], Loss: 61.1255\n",
      "Epoch [300/10000], Loss: 57.7992\n",
      "Epoch [400/10000], Loss: 56.6340\n",
      "Epoch [500/10000], Loss: 56.9259\n",
      "Epoch [600/10000], Loss: 56.4704\n",
      "Epoch [700/10000], Loss: 56.4068\n",
      "Epoch [800/10000], Loss: 56.3196\n",
      "Epoch [900/10000], Loss: 55.5626\n",
      "Epoch [1000/10000], Loss: 56.4000\n",
      "Epoch [1100/10000], Loss: 56.4000\n",
      "Epoch [1200/10000], Loss: 56.4000\n",
      "Epoch [1300/10000], Loss: 56.4000\n",
      "Epoch [1400/10000], Loss: 56.4000\n",
      "Epoch [1500/10000], Loss: 56.4000\n",
      "Epoch [1600/10000], Loss: 56.4000\n",
      "Epoch [1700/10000], Loss: 56.4000\n",
      "Epoch [1800/10000], Loss: 56.4000\n",
      "Epoch [1900/10000], Loss: 56.4000\n",
      "Epoch [2000/10000], Loss: 56.4000\n",
      "Epoch [2100/10000], Loss: 56.4000\n",
      "Epoch [2200/10000], Loss: 56.4000\n",
      "Epoch [2300/10000], Loss: 56.4000\n",
      "Epoch [2400/10000], Loss: 56.4000\n",
      "Epoch [2500/10000], Loss: 56.4000\n",
      "Epoch [2600/10000], Loss: 56.4000\n",
      "Epoch [2700/10000], Loss: 56.4000\n",
      "Epoch [2800/10000], Loss: 56.4000\n",
      "Epoch [2900/10000], Loss: 56.4000\n",
      "Epoch [3000/10000], Loss: 56.4000\n",
      "Epoch [3100/10000], Loss: 56.4000\n",
      "Epoch [3200/10000], Loss: 56.4000\n",
      "Epoch [3300/10000], Loss: 56.4000\n",
      "Epoch [3400/10000], Loss: 56.4000\n",
      "Epoch [3500/10000], Loss: 56.4000\n",
      "Epoch [3600/10000], Loss: 56.4000\n",
      "Epoch [3700/10000], Loss: 56.4000\n",
      "Epoch [3800/10000], Loss: 56.4000\n",
      "Epoch [3900/10000], Loss: 56.4000\n",
      "Epoch [4000/10000], Loss: 56.4000\n",
      "Epoch [4100/10000], Loss: 56.4000\n",
      "Epoch [4200/10000], Loss: 56.4000\n",
      "Epoch [4300/10000], Loss: 56.4000\n",
      "Epoch [4400/10000], Loss: 56.4000\n",
      "Epoch [4500/10000], Loss: 56.4000\n",
      "Epoch [4600/10000], Loss: 56.4000\n",
      "Epoch [4700/10000], Loss: 56.4000\n",
      "Epoch [4800/10000], Loss: 56.4000\n",
      "Epoch [4900/10000], Loss: 56.4000\n",
      "Epoch [5000/10000], Loss: 56.4000\n",
      "Epoch [5100/10000], Loss: 56.4000\n",
      "Epoch [5200/10000], Loss: 56.4000\n",
      "Epoch [5300/10000], Loss: 56.4000\n",
      "Epoch [5400/10000], Loss: 56.4000\n",
      "Epoch [5500/10000], Loss: 56.4000\n",
      "Epoch [5600/10000], Loss: 56.4000\n",
      "Epoch [5700/10000], Loss: 56.4000\n",
      "Epoch [5800/10000], Loss: 56.4000\n",
      "Epoch [5900/10000], Loss: 56.4000\n",
      "Epoch [6000/10000], Loss: 56.4000\n",
      "Epoch [6100/10000], Loss: 56.4000\n",
      "Epoch [6200/10000], Loss: 56.4000\n",
      "Epoch [6300/10000], Loss: 56.4000\n",
      "Epoch [6400/10000], Loss: 56.4000\n",
      "Epoch [6500/10000], Loss: 56.4000\n",
      "Epoch [6600/10000], Loss: 56.4000\n",
      "Epoch [6700/10000], Loss: 56.4000\n",
      "Epoch [6800/10000], Loss: 56.4000\n",
      "Epoch [6900/10000], Loss: 56.4000\n",
      "Epoch [7000/10000], Loss: 56.4000\n",
      "Epoch [7100/10000], Loss: 56.4000\n",
      "Epoch [7200/10000], Loss: 56.4000\n",
      "Epoch [7300/10000], Loss: 56.4000\n",
      "Epoch [7400/10000], Loss: 56.4000\n",
      "Epoch [7500/10000], Loss: 56.4000\n",
      "Epoch [7600/10000], Loss: 56.4000\n",
      "Epoch [7700/10000], Loss: 56.4000\n",
      "Epoch [7800/10000], Loss: 56.4000\n",
      "Epoch [7900/10000], Loss: 56.4000\n",
      "Epoch [8000/10000], Loss: 56.4000\n",
      "Epoch [8100/10000], Loss: 56.4000\n",
      "Epoch [8200/10000], Loss: 56.4000\n",
      "Epoch [8300/10000], Loss: 56.4000\n",
      "Epoch [8400/10000], Loss: 56.4000\n",
      "Epoch [8500/10000], Loss: 56.4000\n",
      "Epoch [8600/10000], Loss: 56.4000\n",
      "Epoch [8700/10000], Loss: 56.4000\n",
      "Epoch [8800/10000], Loss: 56.4000\n",
      "Epoch [8900/10000], Loss: 56.4000\n",
      "Epoch [9000/10000], Loss: 56.4000\n",
      "Epoch [9100/10000], Loss: 56.4000\n",
      "Epoch [9200/10000], Loss: 56.4000\n",
      "Epoch [9300/10000], Loss: 56.4000\n",
      "Epoch [9400/10000], Loss: 56.4000\n",
      "Epoch [9500/10000], Loss: 56.4000\n",
      "Epoch [9600/10000], Loss: 56.4000\n",
      "Epoch [9700/10000], Loss: 56.4000\n",
      "Epoch [9800/10000], Loss: 56.4000\n",
      "Epoch [9900/10000], Loss: 56.4000\n",
      "Epoch [10000/10000], Loss: 56.4000\n",
      "Least-square error on training data (first 10 and last 10): 56.4000\n",
      "Least-square error on test data (10 held out middle data points): 13.4000\n"
     ]
    }
   ],
   "source": [
    "train_X = torch.tensor(np.concatenate((X[:10], X[-10:])), dtype=torch.float32).view(-1, 1)\n",
    "train_Y = torch.tensor(np.concatenate((Y[:10], Y[-10:])), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "test_X = torch.tensor(X[10:20], dtype=torch.float32).view(-1, 1)\n",
    "test_Y = torch.tensor(Y[10:20], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(train_X)\n",
    "    loss = criterion(outputs, train_Y)\n",
    "    loss.backward()  # Added parentheses here\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_pred = net(train_X)\n",
    "    train_error = criterion(train_pred, train_Y)\n",
    "    print(\"Least-square error on training data (first 10 and last 10): {:.4f}\".format(train_error.item()))\n",
    "\n",
    "    test_pred = net(test_X)\n",
    "    test_error = criterion(test_pred, test_Y)\n",
    "    print(\"Least-square error on test data (10 held out middle data points): {:.4f}\".format(test_error.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e298f8b6",
   "metadata": {},
   "source": [
    "##  (iv)\n",
    "### Comparing models fit in HW1 to neural networks in parts (ii) and (iii)\n",
    "\n",
    "The models that were fit to the data in HW1 were Line, Parabola, and a 19th degree polynomial. \n",
    "\n",
    "```\n",
    "The LSE for the Line model on the training data was: 2.242749386808538\n",
    "\n",
    "The LSE for the Line model on the test data was: 3.36363873604787\n",
    "\n",
    "The LSE for the Parabola model on the training data was: 2.1255393482773766\n",
    "\n",
    "The LSE for the Parabola model on the test data was: 8.713651781874919\n",
    "\n",
    "The LSE for the 19th Deg Polynomial on the training data was: 0.028351503968806435\n",
    "\n",
    "The LSE for the 19th Deg Polynomial on the test data was: 28617752784.428474\n",
    "\n",
    "```\n",
    "\n",
    "The line model had the best error for both the training data and the test data. The neural network that was created for this assignment had an error for the training data which is greater than the Line, Parabola, and 19th Deg Polynomial error for the training data. It also had an error for the test data which is also greater than the error for Line and Parabola on the test data, but much less than the error for the 19th deg polynomial on test data. \n",
    "\n",
    "When we change the indices of the training and test data, we get the following results:\n",
    "\n",
    "```\n",
    "The LSE for the Line model on the training data was: 2.8684634748880655\n",
    "\n",
    "The LSE for the Line model on the test data was: 22.197891223912386\n",
    "\n",
    "The LSE for the Parabola model on the training data was: 2.8680459400504987\n",
    "\n",
    "The LSE for the Parabola model on the test data was: 22.571695465713965\n",
    "\n",
    "The LSE for the 19th Deg Polynomial on the training data was: 0.692679558738857\n",
    "\n",
    "The LSE for the 19th Deg Polynomial on the test data was: 154987332439.0542\n",
    "\n",
    "The LSE for the Neural Network on the training data was: 56.4000\n",
    "\n",
    "The LSE for the Neural Network on the test data was: 13.3999\n",
    "```\n",
    "\n",
    "There were some interesting changes in the errors when we alter the indices. The neural network has a much better error on the test data than any of the other models. The line, parabola, and 19th deg polynomial had better LES for the training data than the neural network, but the neural network was by far the best when it comes to the test data. \n",
    "\n",
    "\n",
    "Overall, in the second scenario, where the first 10 and last 10 data points were used for training and the 10 held out middle data points for testing, the neural network had the best performance on the test data compared to the other models. Although the line, parabola, and 19th-degree polynomial had better LSE for the training data than the neural network, the neural network had the lowest error on the test data, suggesting better generalization in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2e4e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mm/bwdqqwfs0y3247yqgd_vb4rr0000gn/T/ipykernel_18991/4127596865.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = y.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# Part Two\n",
    "\n",
    "# Load the MNIST data\n",
    "X, y = mnist.data / 255.0, mnist.target\n",
    "y = y.astype(np.int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bd783dd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of PCA modes: (20, 784)\n"
     ]
    }
   ],
   "source": [
    "# Compute the first 20 PCA modes of the digit images\n",
    "pca = PCA(n_components=20)\n",
    "pca.fit(X)\n",
    "\n",
    "# Get the first 20 PCA modes (principal components)\n",
    "pca_modes = pca.components_\n",
    "\n",
    "print(\"Shape of PCA modes:\", pca_modes.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f3e2036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-forward neural network accuracy: 0.9733\n",
      "LSTM Accuracy: 0.9862857142857143\n",
      "SVM accuracy: 0.9764\n",
      "Decision tree accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to numpy arrays\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Convert the data to tensors\n",
    "X_train_torch = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_torch = torch.tensor(y_train_np, dtype=torch.long)\n",
    "X_test_torch = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for the training data\n",
    "train_data = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Instantiate the neural network, set up the loss function and optimizer\n",
    "ffnn = FFNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(ffnn.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ffnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the neural network\n",
    "with torch.no_grad():\n",
    "    y_pred_ffnn = ffnn(X_test_torch).argmax(dim=1).numpy()\n",
    "    ffnn_accuracy = accuracy_score(y_test, y_pred_ffnn)\n",
    "    print(\"Feed-forward neural network accuracy: {:.4f}\".format(ffnn_accuracy))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "# LSTM model parameters\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "\n",
    "# Reshaping input data for the LSTM model\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "X_train_LSTM = X_train_torch.view(-1, sequence_length, input_size)\n",
    "X_test_LSTM = X_test_torch.view(-1, sequence_length, input_size)\n",
    "\n",
    "lstm_model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# Set the loss function and optimizer for LSTM\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the LSTM model\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(X_train_LSTM), batch_size):\n",
    "        inputs = X_train_LSTM[i:i + batch_size].to(device)\n",
    "        labels = y_train_torch[i:i + batch_size].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "# Test the LSTM model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0, len(X_test_LSTM), batch_size):\n",
    "        inputs = X_test_LSTM[i:i + batch_size].to(device)\n",
    "        labels = y_test_torch[i:i + batch_size].to(device)\n",
    "        output = lstm_model(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'LSTM Accuracy: {correct / total}')\n",
    "    \n",
    "# SVM fit and test   \n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM accuracy: {:.4f}\".format(svm_accuracy))\n",
    "\n",
    "# Decision tree fit and test\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision tree accuracy: {:.4f}\".format(dt_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9314c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230146fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
